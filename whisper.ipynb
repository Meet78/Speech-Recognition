{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f680a1d6-0e66-494e-9c91-182015163dd2",
   "metadata": {
    "id": "f680a1d6-0e66-494e-9c91-182015163dd2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, Audio\n",
    "\n",
    "# Load full CSV\n",
    "df = pd.read_csv(\"cv-valid-train-clean.csv\")\n",
    "\n",
    "# Replace .mp3 with .wav\n",
    "df[\"filename\"] = df[\"file_name\"].str.replace(\".mp3\", \".wav\", regex=False)\n",
    "\n",
    "# Optional: Keep only existing files (recommended)\n",
    "import os\n",
    "df = df[df[\"filename\"].apply(os.path.exists)].reset_index(drop=True)\n",
    "\n",
    "# Randomly sample 2,000 examples\n",
    "df_sampled = df.sample(n=2000, random_state=2).reset_index(drop=True)\n",
    "\n",
    "# Rename for Hugging Face\n",
    "df_sampled = df_sampled.rename(columns={\"filename\": \"audio\", \"text\": \"sentence\"})\n",
    "\n",
    "\n",
    "dataset = Dataset.from_pandas(df_sampled)\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bdcf6b-3d67-4150-9be7-1006c2b61883",
   "metadata": {
    "id": "36bdcf6b-3d67-4150-9be7-1006c2b61883"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_text(batch):\n",
    "    text = batch[\"sentence\"].lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # removes punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # clean up extra whitespace\n",
    "    batch[\"text\"] = text\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(normalize_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb850c6-5004-499a-9372-91cf8b190fdb",
   "metadata": {
    "id": "cbb850c6-5004-499a-9372-91cf8b190fdb"
   },
   "outputs": [],
   "source": [
    "from transformers import WhisperFeatureExtractor, WhisperTokenizer\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"en\", task=\"transcribe\")\n",
    "\n",
    "def preprocess_safe(example):\n",
    "    try:\n",
    "        audio = example[\"audio\"]\n",
    "        inputs = feature_extractor(audio[\"array\"], sampling_rate=16000, return_tensors=\"pt\")\n",
    "        labels = tokenizer(example[\"sentence\"], return_tensors=\"pt\").input_ids\n",
    "        return {\n",
    "            \"input_features\": inputs.input_features[0],\n",
    "            \"labels\": labels[0]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping file due to error: {e}\")\n",
    "        return {}  # Drop this sample\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0cae48-a83a-4c11-96f1-8351e5eeccd2",
   "metadata": {
    "id": "ad0cae48-a83a-4c11-96f1-8351e5eeccd2"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.1)  # 90% train / 10% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7405762c-b468-4623-aaec-d45123b7d5bb",
   "metadata": {
    "id": "7405762c-b468-4623-aaec-d45123b7d5bb"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "    preprocess_safe,  # or preprocess if it works now\n",
    "    remove_columns=[\"file_name\", \"sentence\", \"audio\"],  # explicitly remove them\n",
    "    num_proc=1,\n",
    "    batched=False,\n",
    "    desc=\"Preprocessing\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c360ffc-9133-41c0-bc13-60d7c7b827db",
   "metadata": {
    "id": "3c360ffc-9133-41c0-bc13-60d7c7b827db"
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class WhisperDataCollator:\n",
    "    processor: Any\n",
    "    return_tensors: str = \"pt\"\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        # Convert input_features to tensors\n",
    "        input_features = [torch.tensor(f[\"input_features\"]) for f in features]\n",
    "\n",
    "        # Stack them into a batch\n",
    "        batch = {\n",
    "            \"input_features\": torch.stack(input_features)\n",
    "        }\n",
    "\n",
    "        # Tokenized labels may still be lists of ints â€” pad and convert\n",
    "        label_batch = self.processor.tokenizer.pad(\n",
    "            {\"input_ids\": [f[\"labels\"] for f in features]},\n",
    "            padding=self.padding,\n",
    "            return_tensors=self.return_tensors,\n",
    "        )\n",
    "\n",
    "        # Replace padding token with -100 so it's ignored in loss\n",
    "        label_batch[\"input_ids\"][label_batch[\"input_ids\"] == self.processor.tokenizer.pad_token_id] = -100\n",
    "        batch[\"labels\"] = label_batch[\"input_ids\"]\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc6d52b-9b51-41f1-adef-b0088d0f62b6",
   "metadata": {
    "id": "7fc6d52b-9b51-41f1-adef-b0088d0f62b6"
   },
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration, DataCollatorForSeq2Seq, WhisperProcessor\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\").to(\"cuda\")\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"en\", task=\"transcribe\")\n",
    "\n",
    "\n",
    "\n",
    "data_collator = WhisperDataCollator(processor=processor)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b5121e-30b0-4da9-8d0d-fad554f8e1a2",
   "metadata": {
    "id": "a6b5121e-30b0-4da9-8d0d-fad554f8e1a2"
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # Convert to numpy if needed\n",
    "    if isinstance(pred_ids, tuple):\n",
    "        pred_ids = pred_ids[0]\n",
    "\n",
    "    # Decode the entire predicted sentence (for comparison)\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True, temperature=0.9)\n",
    "\n",
    "    # Replace -100 in the labels before decoding\n",
    "    label_ids = np.where(label_ids != -100, label_ids, tokenizer.pad_token_id)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True, temperature=0.9)\n",
    "\n",
    "    # Manually check the token IDs for the first few predictions and labels\n",
    "    i = random.randint(1,199)\n",
    "    print(f\"Prediction {i}: {pred_str[i]}\")\n",
    "\n",
    "    print(f\"Label {i}: {label_str[i]}\")\n",
    "\n",
    "    # Compute WER\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d017ef-1dcb-44c7-a210-f927bf5a8a04",
   "metadata": {
    "id": "40d017ef-1dcb-44c7-a210-f927bf5a8a04"
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-finetuned_4-18\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_steps = 100,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=10,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efadd5f-48c9-4d61-b395-5e0fa48c7f5e",
   "metadata": {
    "id": "7efadd5f-48c9-4d61-b395-5e0fa48c7f5e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()  # Clears memory that is no longer needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137e8237-4eb3-44e2-9eda-c9cbde0ed25f",
   "metadata": {
    "id": "137e8237-4eb3-44e2-9eda-c9cbde0ed25f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "from transformers import WhisperProcessor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    processing_class=processor,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724715de-eff7-4152-82aa-02d24d99949b",
   "metadata": {
    "id": "724715de-eff7-4152-82aa-02d24d99949b"
   },
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "def load_audio(file_path, target_sr=16000):\n",
    "    waveform, sample_rate = torchaudio.load(file_path)\n",
    "    if sample_rate != target_sr:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=target_sr)\n",
    "        waveform = resampler(waveform)\n",
    "    return waveform.squeeze().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8216f17e-3d56-49a7-ba08-8f9e676afede",
   "metadata": {
    "id": "8216f17e-3d56-49a7-ba08-8f9e676afede"
   },
   "outputs": [],
   "source": [
    "def transcribe(file_path):\n",
    "    model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language = \"en\", task = \"transcribe\")\n",
    "    model.config.suppress_tokens = []\n",
    "    # Load and resample\n",
    "    audio = load_audio(file_path)\n",
    "\n",
    "    # Preprocess\n",
    "    inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(inputs.input_features)\n",
    "\n",
    "    # Decode\n",
    "    transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return transcription\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5810c122-eb18-43b0-b429-64b0bac3201c",
   "metadata": {
    "id": "5810c122-eb18-43b0-b429-64b0bac3201c"
   },
   "outputs": [],
   "source": [
    "text = transcribe(\"cv-valid-train/sample-000031.wav\")\n",
    "print(\"Transcription:\", text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b36dd72-08c6-4515-a86b-6419bf2059b3",
   "metadata": {
    "id": "5b36dd72-08c6-4515-a86b-6419bf2059b3"
   },
   "outputs": [],
   "source": [
    "# Save both feature extractor + tokenizer together\n",
    "processor.save_pretrained(\"./whisper-finetuned_final_4-15\")\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained(\"./whisper-finetuned_final_4-15\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0309b9bf-8dcc-4711-a55b-4c4a9c978aac",
   "metadata": {
    "id": "0309b9bf-8dcc-4711-a55b-4c4a9c978aac"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "knn",
   "language": "python",
   "name": "knn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
